{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.10).\n",
      "Path to dataset files: C:\\Users\\Francisco\\.cache\\kagglehub\\datasets\\oktayrdeki\\heart-disease\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"oktayrdeki/heart-disease\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"./data/heart_disease.csv\")\n",
    "\n",
    "# Display first few rows\n",
    "#print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\francisco\\tech_projects\\ece143_project\\heart-disease-analysis\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\francisco\\tech_projects\\ece143_project\\heart-disease-analysis\\venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\francisco\\tech_projects\\ece143_project\\heart-disease-analysis\\venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\francisco\\tech_projects\\ece143_project\\heart-disease-analysis\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\francisco\\tech_projects\\ece143_project\\heart-disease-analysis\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\francisco\\tech_projects\\ece143_project\\heart-disease-analysis\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\francisco\\tech_projects\\ece143_project\\heart-disease-analysis\\venv\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\francisco\\tech_projects\\ece143_project\\heart-disease-analysis\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\francisco\\tech_projects\\ece143_project\\heart-disease-analysis\\venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\francisco\\tech_projects\\ece143_project\\heart-disease-analysis\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# All imports needed\n",
    "%pip install pandas numpy scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame object of the csv file passed in.\n",
    "\n",
    "    :param file_path: String of the file path to load in\n",
    "\n",
    "    :return: A DataFrame object of the csv data\n",
    "    \"\"\"\n",
    "    assert(isinstance(file_path, str)), \"File path must be a valid path\"\n",
    "    # file_path = \"./data/heart_disease.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_info(data_frame):\n",
    "    \"\"\"\n",
    "    View the structure of the data frame\n",
    "\n",
    "    :param data_frame: The data frame to get the structure of\n",
    "    \"\"\"\n",
    "    assert(isinstance(data_frame, pd.DataFrame)), \"The input must be DataFrame object\"\n",
    "    print(\"Summary of Dataset:\")\n",
    "    data_frame.info()\n",
    "    print(\"Get missing count\")\n",
    "    data_frame.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_rows(data_frame: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Get number of rows of the data frame\n",
    "\n",
    "    :param data_frame: The data frame to get the number of rows\n",
    "    \"\"\"\n",
    "    assert(isinstance(data_frame, pd.DataFrame)), \"The input must be DataFrame object\"\n",
    "    return data_frame.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_cols(data_frame: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Get number of columns of the data frame\n",
    "\n",
    "    :param data_frame: The data frame to get the number of columns\n",
    "    \"\"\"\n",
    "    assert(isinstance(data_frame, pd.DataFrame)), \"The input must be DataFrame object\"\n",
    "    return data_frame.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_non_numerical_columns(data_frame):\n",
    "    \"\"\"\n",
    "    Classification model that predicts the output of non-numerical\n",
    "    data in the data frame for missing entries.\n",
    "\n",
    "    :param data_Frame: The data_frame.\n",
    "\n",
    "    :return: A new data frame with all of the classified columns.\n",
    "    \"\"\"\n",
    "    assert(isinstance(data_frame, pd.DataFrame)), \"The input must be DataFrame object\"\n",
    "    non_numeric_cols = data_frame.select_dtypes(exclude=['number']).columns\n",
    "    for col in non_numeric_cols:\n",
    "        label_encoder = LabelEncoder()\n",
    "        data_frame[col] = label_encoder.fit_transform(data_frame[col])\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_random_rows_no_heart_disease(data_frame: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Drop random rows from the given pandas DataFrame.\n",
    "    Used to drop random entries that do not have heart disease.\n",
    "    This is to balance the dataset. There are about 2000\n",
    "    with heart disease and 8000 without heart disease.\n",
    "    This is skewing the data visualizations and prediction model\n",
    "    to just 80% not having heart disease.\n",
    "\n",
    "    :param df: The pandas DataFrame if the number of\n",
    "    no heart disease and heart disease do not match.\n",
    "    \"\"\"\n",
    "    assert(isinstance(data_frame, pd.DataFrame) and data_frame.shape[0] > 0), \"Argument must be a valid non-empty pandas DataFrame\"\n",
    "    assert(\"Heart Disease Status\" in data_frame.columns), \"There must be a Heart Disease Status column in the pandas DataFrame\"\n",
    "    ser = data_frame[\"Heart Disease Status\"].value_counts()\n",
    "    num_heart_disease, num_no_heart_disease = ser[\"Yes\"], ser[\"No\"]\n",
    "    assert(num_heart_disease > 0 and num_no_heart_disease > 0), \"Number of Yes and No Heart Disease Status in the DataFrame should be greater than 0. If not, not a good dataset\"\n",
    "    if num_heart_disease > num_no_heart_disease:\n",
    "        num_to_drop = num_heart_disease - num_no_heart_disease\n",
    "        indices_to_drop = data_frame[data_frame[\"Heart Disease Status\"].str.lower() == \"yes\"].sample(num_to_drop).index\n",
    "        data_frame = data_frame.drop(indices_to_drop)\n",
    "    elif num_no_heart_disease > num_heart_disease:\n",
    "        num_to_drop = num_no_heart_disease - num_heart_disease\n",
    "        indices_to_drop = data_frame[data_frame[\"Heart Disease Status\"].str.lower() == \"no\"].sample(num_to_drop).index\n",
    "        data_frame = data_frame.drop(indices_to_drop)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputerMethod(Enum):\n",
    "    KNN = \"KNN\"\n",
    "    SIMPLE = \"Simple\"\n",
    "    DROP = \"Drop\"\n",
    "\n",
    "def clean_data(data_frame: pd.DataFrame, method: ImputerMethod):\n",
    "    \"\"\" \n",
    "    Clean the data up from any missing values (if any) by just dropping\n",
    "    these rows or by using KNN Imputer on numerical columns and\n",
    "    Simple Imputer on non-numerical columns. Produces a cleaned\n",
    "    data frame without missing entries. \n",
    "\n",
    "    :param data_frame: The data frame to clean up\n",
    "\n",
    "    :param method: The method of cleaning the data. Can either drop all rows with missing entries,\n",
    "    or use KNN Imputer on numerical columns and simple Imputer on non-numericla columns.\n",
    "    Method is of type ImputerMethod enum.\n",
    "\n",
    "    :return: The cleaned data frame if there are any rows that have missing entries\n",
    "    \"\"\"\n",
    "    assert(isinstance(data_frame, pd.DataFrame)), \"The input must be DataFrame object\"\n",
    "    assert(isinstance(method, ImputerMethod)), \"The input must be an imputer method either KNN or SIMPLE\"\n",
    "\n",
    "    if data_frame.isnull().any(axis=1).sum():\n",
    "        if method == ImputerMethod.KNN:\n",
    "            # KNNImputer only works on numerical data\n",
    "            # Apply to numerical columns with missing values\n",
    "            numerical_cols = data_frame.select_dtypes(include=['number']).columns\n",
    "            knn_imputer = KNNImputer(n_neighbors=5)\n",
    "            data_frame[numerical_cols] = knn_imputer.fit_transform(data_frame[numerical_cols])\n",
    "\n",
    "            # Apply SimpleImputer for non-numerical columns\n",
    "            non_numerical_cols = data_frame.select_dtypes(exclude=['number']).columns\n",
    "            mode_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "            data_frame[non_numerical_cols] = mode_imputer.fit_transform(data_frame[non_numerical_cols])\n",
    "        elif method == ImputerMethod.DROP:\n",
    "            data_frame = data_frame.dropna()\n",
    "    #data_frame = drop_random_rows_no_heart_disease(data_frame)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    \"\"\"\n",
    "    Main function to run the complete data processing/cleaning.\n",
    "\n",
    "    :return: df_cleaned_drop, df_cleaned_knn which are the\n",
    "    cleaned up datasets with their respective methods.\n",
    "    \"\"\"\n",
    "    df = load_data(\"../data/heart_disease.csv\")\n",
    "    print(f\"Num rows before cleaning: {get_num_rows(df)}\")\n",
    "    print(f\"Num of cols before cleaning: {get_num_cols(df)}\\n\")\n",
    "\n",
    "    df_cleaned_drop = clean_data(df, ImputerMethod.DROP)\n",
    "    print(f\"Num of rows after cleaning with DROP: {get_num_rows(df_cleaned_drop)}\")\n",
    "    print(f\"Num of cols after cleaning with DROP: {get_num_cols(df_cleaned_drop)}\")\n",
    "    duplicate_counts = df_cleaned_drop.duplicated().sum()\n",
    "    print(f\"Duplicate rows in cleaned DROP: {duplicate_counts}\\n\")\n",
    "\n",
    "    df_cleaned_knn = clean_data(df, ImputerMethod.KNN)\n",
    "    print(f\"Num of rows after cleaning with KNN: {get_num_rows(df_cleaned_knn)}\")\n",
    "    print(f\"Num of cols after cleaning with KNN: {get_num_cols(df_cleaned_knn)}\")\n",
    "    duplicate_counts = df_cleaned_knn.duplicated().sum()\n",
    "    print(f\"Duplicate rows in cleaned KNN: {duplicate_counts}\\n\")\n",
    "    \n",
    "    return df_cleaned_drop, df_cleaned_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num rows before cleaning: 10000\n",
      "Num of cols before cleaning: 21\n",
      "\n",
      "Num of rows after cleaning with DROP: 7067\n",
      "Num of cols after cleaning with DROP: 21\n",
      "Duplicate rows in cleaned DROP: 0\n",
      "\n",
      "Num of rows after cleaning with KNN: 10000\n",
      "Num of cols after cleaning with KNN: 21\n",
      "Duplicate rows in cleaned KNN: 0\n",
      "\n",
      "Gender\n",
      "Male      3564\n",
      "Female    3503\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of men in clean with DROP: 3564\n",
      "Number of women in clean with DROP: 3503\n",
      "\n",
      "Gender\n",
      "Male      5022\n",
      "Female    4978\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of men in clean with KNN: 5022\n",
      "Number of women in clean with KNN: 4978\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_drop, df_cleaned_knn = run()\n",
    "\n",
    "cleaned_drop_series = df_cleaned_drop[\"Gender\"].value_counts()\n",
    "print(f\"{cleaned_drop_series}\\n\")\n",
    "num_men_drop, num_women_drop = cleaned_drop_series\n",
    "print(f\"Number of men in clean with DROP: {num_men_drop}\")\n",
    "print(f\"Number of women in clean with DROP: {num_women_drop}\\n\")\n",
    "\n",
    "cleaned_knn_series = df_cleaned_knn[\"Gender\"].value_counts()\n",
    "print(f\"{cleaned_knn_series}\\n\")\n",
    "num_men_knn, num_women_knn = cleaned_knn_series\n",
    "print(f\"Number of men in clean with KNN: {num_men_knn}\")\n",
    "print(f\"Number of women in clean with KNN: {num_women_knn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart Disease Status\n",
      "No     2000\n",
      "Yes    2000\n",
      "Name: count, dtype: int64\n",
      "Stress Level\n",
      "Medium    1376\n",
      "High      1320\n",
      "Low       1304\n",
      "Name: count, dtype: int64\n",
      "Alcohol Consumption\n",
      "Medium    2016\n",
      "High      1004\n",
      "Low        980\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Use the KNN_Imputer One and get equal distrubtion to prevent bias in dataset\n",
    "cleaned_data = drop_random_rows_no_heart_disease(df_cleaned_knn)\n",
    "print(cleaned_data[\"Heart Disease Status\"].value_counts())\n",
    "# Create a new column that quantizes the Cholesterol Levels\n",
    "cleaned_data[\"Cholesterol Category\"] = pd.cut(\n",
    "    cleaned_data[\"Cholesterol Level\"],\n",
    "    bins=[0, 199, 239, float(\"inf\")],  \n",
    "    labels=[\"Normal\", \"Elevated\", \"High\"],  \n",
    "    right=True  \n",
    ")\n",
    "# Create a new column that quantizes the BMI ranges\n",
    "cleaned_data[\"BMI Category\"] = pd.cut(\n",
    "    cleaned_data[\"BMI\"],\n",
    "    bins=[0, 18.5, 25, 30, float(\"inf\")],  \n",
    "    labels=[\"Underweight\", \"Normal\", \"Overweight\", \"Obese\"],  \n",
    "    right=True  # Include right edge in the bin\n",
    ")\n",
    "# Create a new column that quantizes the Blood Pressure ranges\n",
    "cleaned_data[\"Blood Pressure Category\"] = pd.cut(\n",
    "    cleaned_data[\"Blood Pressure\"],\n",
    "    bins=[0, 120, 129, 139, float(\"inf\")], \n",
    "    labels=[\"Normal\", \"Elevated\", \"High\", \"Very High\"],  \n",
    "    right=True  \n",
    ")\n",
    "\n",
    "print(cleaned_data[\"Stress Level\"].value_counts())\n",
    "print(cleaned_data[\"Alcohol Consumption\"].value_counts())\n",
    "\n",
    "cleaned_data.to_csv(\"../data/equal_distribution_hds.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
